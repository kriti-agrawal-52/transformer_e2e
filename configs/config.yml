# config.yml
# Configuration for Transformer Learning
# --- CONTROL FLAGS ---

SHOULD_TRAIN_SINGLE_RUN: True # Set to true to run the single run (where we have 1 set of hyperparameters)
SHOULD_HYPERPARAMETER_SEARCH: False # Set to true for hyperparameter searching
ALWAYS_LOG_ARTIFACTS: True # Flag for if we want to save our model as wandb artefact

# --- Environment ---
DEVICE: "auto" # "auto", "cuda", or "cpu"

# --- Dataset and Tokenizer ---
DATASET_NAME: "wikitext"
DATASET_VARIANT: "wikitext-2-raw-v1"
TOKENIZER_NAME: "gpt2"

# --- Data Loading Configuration ---
# Set the maximum number of characters to use for training
# This limits the dataset size for faster iteration, especially for tuning.
RAW_TEXT_LIMIT: 10000000 # 10 Million characters

RUN_MODE: "single" # single or grid

# --- TRAINING HYPERPARAMETERS FOR SINGLE RUN TRAINING---
BATCH_SIZE: 32
CONTEXT_WINDOW: 256
CHANNEL_DIM: 512
NUM_HEADS: 8
NUM_LAYERS: 8
LEARNING_RATE: 3e-4
TRAINING_STEPS: 30000

MIN_DELTA: 0.001 # we should not make this value too small because we want to have meaningful improvement in our validation loss
EVAL_ITERS_VAL: 10 # for calculating evaluation loss, we randomly take EVAL_ITERS_VAL batches of validation set, and take their average loss.
EVAL_ITERS_TEST: 20
VALIDATION_CHECK_EVERY: 100
DROPOUT_RATE: 0.1
FINAL_DROPOUT_MULTIPLIER: 2.0
MAX_DROPOUT_VAL: 0.25
EARLY_STOPPING_PATIENCE: 50
MIN_SUCCESSFUL_VAL_BATCH_RATIO: 0.5 # 0.5 means that we need at least 50%
# of the validation batches to be successful to continue training

# --- TRAINING HYPERPARAMETERS GRID SEARCH FOR HYPERPARAMETER SEARCH ---
HP_SEARCH_LRS: [3e-4, 1e-4]
HP_SEARCH_BATCH_SIZES: [8, 16]
HP_SEARCH_CONTEXT_WINDOWS: [32, 64]

# FIXED HYPERPARAMETERS FOR HYPERPARAMETER SEARCH
HP_SEARCH_STEPS: 5000
HP_VALIDATION_CHECK_EVERY: 25
HP_EARLY_STOPPING_PATIENCE: 10

# --- WANDB CONFIGURATIONS ---
WANDB_PROJECT: "Transformer_e2e"
WANDB_RUN_PREFIX:
  - "transformer_e2e_single_run_"
  - "transformer_e2e_sweep_"

# --- PATH FOR SAVING MODEL CHECKPOINTS ---
MODEL_CHECKPOINTS_DIR: "model_checkpoints/" # Path for saving model checkpoints
LOG_FILE: "transformer_epic_model.log" # file to log runs on the codebase
LOSS_PLOT_DIRECTORY: "train_eval_plots/" # directory for loss plots

# --- TRAINING MANAGEMENT ---
DELETE_LATEST_CHECKPOINT_ON_COMPLETION: True # Set to True to enable deletion, False to keep

